<div align="center">

ğŸ‡«ğŸ‡· [Version franÃ§aise](#french) | ğŸ‡¬ğŸ‡§ [English version](#english)

</div>

---

<a name="french"></a>

# ğŸ§  NeuroTriple-Lab

> **Laboratoire PÃ©dagogique de RÃ©seaux de Neurones**
> DÃ©montrer que plus on entraÃ®ne une IA, plus elle est prÃ©cise dans ses rÃ©ponses.

---

## ğŸ“‹ Description

**NeuroTriple-Lab** est un projet Ã©ducatif interactif qui utilise un rÃ©seau de neurones simple pour apprendre la fonction mathÃ©matique `f(x) = 3x` (tripler un nombre).

Le but principal est de **dÃ©montrer de maniÃ¨re visuelle et mesurable** que :

- âœ… **Plus on augmente le nombre d'epochs** (itÃ©rations d'entraÃ®nement), plus le modÃ¨le est prÃ©cis
- âœ… **Plus on ajoute de couches intermÃ©diaires** (profondeur du rÃ©seau), plus le modÃ¨le peut apprendre efficacement
- âœ… L'architecture et la durÃ©e d'entraÃ®nement sont des facteurs clÃ©s de la performance d'une IA

---

## ğŸ¯ Concept de Base

Le code original est simple : un rÃ©seau de neurones reÃ§oit des nombres en entrÃ©e et doit apprendre Ã  prÃ©dire leur triple.

```python
# DonnÃ©es d'entraÃ®nement
entree = [1, 2, 3, 4, 5]       # Nombres en entrÃ©e
sortie = [3, 6, 9, 12, 15]     # Le triple (rÃ©sultat attendu)
```

Le rÃ©seau n'a **aucune connaissance prÃ©alable** de la multiplication. Il apprend uniquement Ã  partir des exemples fournis, en ajustant ses poids internes Ã  chaque epoch.

---

## ğŸš€ 5 FonctionnalitÃ©s PÃ©dagogiques

### 1. ğŸ“Š Comparaison du Nombre d'Epochs

EntraÃ®ne le mÃªme modÃ¨le avec diffÃ©rents nombres d'epochs (ex: 10, 50, 100, 500, 1000, 3000) et compare la prÃ©cision obtenue.

**Ce que vous apprenez** : L'impact direct du nombre d'itÃ©rations sur la qualitÃ© de l'apprentissage.

### 2. ğŸ—ï¸ Comparaison des Architectures

Teste diffÃ©rentes profondeurs de rÃ©seau :
- Simple : 1 couche, 8 neurones
- Moyenne : 1 couche, 64 neurones
- Profonde : 3 couches (128â†’64â†’32)
- TrÃ¨s profonde : 5 couches (256â†’128â†’64â†’32â†’16)

### 3. ğŸ“ˆ Visualisation de l'Apprentissage

Affiche en dÃ©tail la courbe d'apprentissage (loss) avec trois graphiques.

### 4. ğŸ“‹ Tableau de PrÃ©cision DÃ©taillÃ©

Combine diffÃ©rents epochs ET architectures pour produire un tableau comparatif complet.

### 5. ğŸ”¬ Mode ExpÃ©rimentation AutomatisÃ©e

Lance une batterie complÃ¨te de tests (16 combinaisons) et produit un classement du meilleur au moins bon.

### ğŸ® Mode Interactif (Bonus)

AprÃ¨s l'entraÃ®nement, entrez n'importe quel nombre et voyez la prÃ©diction de l'IA en temps rÃ©el.

---

## ğŸ’» Installation

### PrÃ©requis

- Python 3.8 ou supÃ©rieur
- pip (gestionnaire de paquets Python)

### Ã‰tapes

```bash
# 1. Cloner le dÃ©pÃ´t
git clone https://github.com/thierrymaesen/NeuroTriple-Lab.git

# 2. AccÃ©der au dossier
cd NeuroTriple-Lab

# 3. Installer les dÃ©pendances
pip install -r requirements.txt

# 4. Lancer le programme
python neurotriplelab.py
```

---

## ğŸ“Š Exemples de RÃ©sultats

| Epochs | Erreur Moyenne | QualitÃ© |
|--------|---------------|---------|
| 10     | ~150.0000     | Insuffisant |
| 50     | ~25.0000      | Moyen |
| 100    | ~8.0000       | Bon |
| 500    | ~0.5000       | TrÃ¨s bon |
| 1000   | ~0.0500       | Excellent |
| 3000   | ~0.0010       | Excellent |

---

## ğŸ“œ Licence

Ce projet est sous licence **MIT** - voir le fichier [LICENSE](LICENSE) pour plus de dÃ©tails.

## ğŸ¤ Contribuer

Les contributions sont les bienvenues !

1. Fork le projet
2. CrÃ©er une branche pour votre fonctionnalitÃ©
3. Commiter vos changements
4. Pousser la branche
5. Ouvrir une Pull Request

---

<p align="center">
  <strong>ğŸ§  Plus on entraÃ®ne, plus l'IA est prÃ©cise ! ğŸ¯</strong>
</p>

---

<a name="english"></a>

# ğŸ§  NeuroTriple-Lab

> **Educational Neural Network Laboratory**
> Demonstrating that the more you train an AI, the more accurate it becomes.

---

## ğŸ“‹ Description

**NeuroTriple-Lab** is an interactive educational project that uses a simple neural network to learn the mathematical function `f(x) = 3x` (tripling a number).

The main goal is to **visually and measurably demonstrate** that:

- âœ… **The more you increase the number of epochs** (training iterations), the more accurate the model becomes
- âœ… **The more intermediate layers you add** (network depth), the more efficiently the model can learn
- âœ… Architecture and training duration are key factors in AI performance

---

## ğŸ¯ Core Concept

The original code is simple: a neural network receives numbers as input and must learn to predict their triple.

```python
# Training data
input_data = [1, 2, 3, 4, 5]        # Input numbers
output_data = [3, 6, 9, 12, 15]     # The triple (expected result)
```

The network has **no prior knowledge** of multiplication. It learns solely from the provided examples, adjusting its internal weights at each epoch.

---

## ğŸš€ 5 Educational Features

### 1. ğŸ“Š Epoch Count Comparison

Trains the same model with different numbers of epochs (e.g., 10, 50, 100, 500, 1000, 3000) and compares the accuracy achieved.

**What you learn**: The direct impact of iteration count on learning quality.

### 2. ğŸ—ï¸ Architecture Comparison

Tests different network depths:
- Simple: 1 layer, 8 neurons
- Medium: 1 layer, 64 neurons
- Deep: 3 layers (128â†’64â†’32)
- Very deep: 5 layers (256â†’128â†’64â†’32â†’16)

### 3. ğŸ“ˆ Learning Visualization

Displays the learning curve (loss) in detail with three graphs.

### 4. ğŸ“‹ Detailed Accuracy Table

Combines different epochs AND architectures to produce a complete comparative table.

### 5. ğŸ”¬ Automated Experimentation Mode

Launches a complete battery of tests (16 combinations) and produces a ranking from best to worst.

### ğŸ® Interactive Mode (Bonus)

After training, enter any number and see the AI prediction in real time.

---

## ğŸ’» Installation

### Prerequisites

- Python 3.8 or higher
- pip (Python package manager)

### Steps

```bash
# 1. Clone the repository
git clone https://github.com/thierrymaesen/NeuroTriple-Lab.git

# 2. Navigate to the folder
cd NeuroTriple-Lab

# 3. Install dependencies
pip install -r requirements.txt

# 4. Run the program
python neurotriplelab.py
```

---

## ğŸ“Š Example Results

| Epochs | Average Error | Quality |
|--------|--------------|---------|
| 10     | ~150.0000    | Insufficient |
| 50     | ~25.0000     | Average |
| 100    | ~8.0000      | Good |
| 500    | ~0.5000      | Very Good |
| 1000   | ~0.0500      | Excellent |
| 3000   | ~0.0010      | Excellent |

---

## ğŸ“œ License

This project is licensed under the **MIT** License - see the [LICENSE](LICENSE) file for details.

## ğŸ¤ Contributing

Contributions are welcome!

1. Fork the project
2. Create a branch for your feature
3. Commit your changes
4. Push the branch
5. Open a Pull Request

---

<p align="center">
  <strong>ğŸ§  The more you train, the more accurate the AI becomes! ğŸ¯</strong>
</p><div align="center">

ğŸ‡«ğŸ‡· [Version franÃ§aise](#french) | ğŸ‡¬ğŸ‡§ [English version](#english)

</div>

---

<a name="french"></a>

# ğŸ§  NeuroTriple-Lab

> **Laboratoire PÃ©dagogique de RÃ©seaux de Neurones**
> > DÃ©montrer que plus on entraÃ®ne une IA, plus elle est prÃ©cise dans ses rÃ©ponses.
> >
> > ---
> >
> > ## ğŸ“‹ Description
> >
> > **NeuroTriple-Lab** est un projet Ã©ducatif interactif qui utilise un rÃ©seau de neurones simple pour apprendre la fonction mathÃ©matique `f(x) = 3x` (tripler un nombre).
> >
> > Le but principal est de **dÃ©montrer de maniÃ¨re visuelle et mesurable** que :
> >
> > - âœ… **Plus on augmente le nombre d'epochs** (itÃ©rations d'entraÃ®nement), plus le modÃ¨le est prÃ©cis
> > - - âœ… **Plus on ajoute de couches intermÃ©diaires** (profondeur du rÃ©seau), plus le modÃ¨le peut apprendre efficacement
> >   - - âœ… L'architecture et la durÃ©e d'entraÃ®nement sont des facteurs clÃ©s de la performance d'une IA
> >    
> >     - ---
> >
> > ## ğŸ¯ Concept de Base
> >
> > Le code original est simple : un rÃ©seau de neurones reÃ§oit des nombres en entrÃ©e et doit apprendre Ã  prÃ©dire leur triple.
> >
> > ```python
> > # DonnÃ©es d'entraÃ®nement
> > entree = [1, 2, 3, 4, 5]       # Nombres en entrÃ©e
> > sortie = [3, 6, 9, 12, 15]     # Le triple (rÃ©sultat attendu)
> > ```
> >
> > Le rÃ©seau n'a **aucune connaissance prÃ©alable** de la multiplication. Il apprend uniquement Ã  partir des exemples fournis, en ajustant ses poids internes Ã  chaque epoch.
> >
> > ---
> >
> > ## ğŸš€ 5 FonctionnalitÃ©s PÃ©dagogiques
> >
> > ### 1. ğŸ“Š Comparaison du Nombre d'Epochs
> >
> > EntraÃ®ne le mÃªme modÃ¨le avec diffÃ©rents nombres d'epochs (ex: 10, 50, 100, 500, 1000, 3000) et compare la prÃ©cision obtenue. GÃ©nÃ¨re un graphique montrant la courbe de loss et l'erreur finale.
> >
> > **Ce que vous apprenez** : L'impact direct du nombre d'itÃ©rations sur la qualitÃ© de l'apprentissage.
> >
> > ### 2. ğŸ—ï¸ Comparaison des Architectures
> >
> > Teste diffÃ©rentes profondeurs de rÃ©seau :
> > - Simple : 1 couche, 8 neurones
> > - - Moyenne : 1 couche, 64 neurones
> >   - - Profonde : 3 couches (128â†’64â†’32)
> >     - - TrÃ¨s profonde : 5 couches (256â†’128â†’64â†’32â†’16)
> >      
> >       - **Ce que vous apprenez** : Comment le nombre de couches et de neurones influence la capacitÃ© d'apprentissage.
> >      
> >       - ### 3. ğŸ“ˆ Visualisation de l'Apprentissage
> >      
> >       - Affiche en dÃ©tail la courbe d'apprentissage (loss) avec trois graphiques :
> > - Courbe de loss complÃ¨te
> > - - Courbe en Ã©chelle logarithmique
> >   - - PrÃ©dictions du modÃ¨le vs valeurs rÃ©elles
> >    
> >     - **Ce que vous apprenez** : Comment le modÃ¨le rÃ©duit progressivement son erreur au fil de l'entraÃ®nement.
> >    
> >     - ### 4. ğŸ“‹ Tableau de PrÃ©cision DÃ©taillÃ©
> >    
> >     - Combine diffÃ©rents epochs ET architectures pour produire un tableau comparatif complet avec temps d'entraÃ®nement, erreur absolue et statut de qualitÃ©.
> > 
**Ce que vous apprenez** : La relation entre complexitÃ© du modÃ¨le, durÃ©e d'entraÃ®nement et prÃ©cision finale.

### 5. ğŸ”¬ Mode ExpÃ©rimentation AutomatisÃ©e

Lance une batterie complÃ¨te de tests (16 combinaisons) et produit un classement du meilleur au moins bon, avec un graphique rÃ©capitulatif.

**Ce que vous apprenez** : Comment trouver systÃ©matiquement la meilleure configuration pour un problÃ¨me donnÃ©.

### ğŸ® Mode Interactif (Bonus)

AprÃ¨s l'entraÃ®nement, entrez n'importe quel nombre et voyez la prÃ©diction de l'IA en temps rÃ©el, avec l'erreur par rapport Ã  la valeur exacte.

---

## ğŸ’» Installation

### PrÃ©requis

- Python 3.8 ou supÃ©rieur
- - pip (gestionnaire de paquets Python)
 
  - ### Ã‰tapes
 
  - ```bash
    # 1. Cloner le dÃ©pÃ´t
    git clone https://github.com/thierrymaesen/NeuroTriple-Lab.git

    # 2. AccÃ©der au dossier
    cd NeuroTriple-Lab

    # 3. Installer les dÃ©pendances
    pip install -r requirements.txt

    # 4. Lancer le programme
    python neurotriplelab.py
    ```

    ---

    > âš ï¸ **ATTENTION â€” Performance et temps de calcul**
    > >
    > >> Le Deep Learning (apprentissage profond) est **trÃ¨s gourmand en ressources**. Si vous utilisez un ordinateur peu puissant (ancien PC, machine sans GPU), soyez prudent avec les paramÃ¨tres suivants :
    > >> >
    > >> >> - **Nombre d'epochs** : Au-delÃ  de **5000 epochs**, le temps de calcul peut devenir trÃ¨s long. Commencez avec des valeurs modestes (500-1000) et augmentez progressivement.
    > >> >> - > - **Couches denses (Dense layers)** : Plus vous ajoutez de couches et de neurones (ex: `[256, 128, 64, 32, 16]`), plus chaque epoch prend du temps.
    > >> >>   > - > - **Combinaison des deux** : 5000 epochs Ã— 5 couches denses = temps de calcul potentiellement **trÃ¨s Ã©levÃ©** !
    > >> >>   >   > - >
    > >> >>   >   >   >> ğŸ’¡ **Conseil** : Commencez petit (ex: 500 epochs, 1 couche de 64 neurones), observez les rÃ©sultats, puis augmentez graduellement.
    > >> >>   >   >   >>
    > >> >>   >   >   >> ---
    > >> >>   >   >   >>
    > >> >>   >   >   >> ## ğŸ“– Utilisation
    > >> >>   >   >   >>
    > >> >>   >   >   >> Au lancement, un menu interactif s'affiche :
    > >> >>   >   >   >>
    > >> >>   >   >   >> ```
    > >> >>   >   >   >> MENU PRINCIPAL
    > >> >>   >   >   >> ----------------------------------------
    > >> >>   >   >   >> 1. Comparer diffÃ©rents nombres d'Epochs
    > >> >>   >   >   >> 2. Comparer diffÃ©rentes Architectures
    > >> >>   >   >   >> 3. Visualiser l'Apprentissage
    > >> >>   >   >   >> 4. Tableau de PrÃ©cision DÃ©taillÃ©
    > >> >>   >   >   >> 5. Mode ExpÃ©rimentation AutomatisÃ©e
    > >> >>   >   >   >> 6. Mode Interactif (tester le modÃ¨le)
    > >> >>   >   >   >> 0. Quitter
    > >> >>   >   >   >> ----------------------------------------
    > >> >>   >   >   >> Votre choix :
    > >> >>   >   >   >> ```
    > >> >>   >   >   >>
    > >> >>   >   >   >> ---
    > >> >>   >   >   >>
    > >> >>   >   >   >> ## ğŸ“Š Exemples de RÃ©sultats
    > >> >>   >   >   >>
    > >> >>   >   >   >> ### Impact des Epochs sur la PrÃ©cision
    > >> >>   >   >   >>
    > >> >>   >   >   >> | Epochs | Erreur Moyenne | QualitÃ© |
    > >> >>   >   >   >> |--------|---------------|---------|
    > >> >>   >   >   >> | 10     | ~150.0000     | Insuffisant |
    > >> >>   >   >   >> | 50     | ~25.0000      | Moyen |
    > >> >>   >   >   >> | 100    | ~8.0000       | Bon |
    > >> >>   >   >   >> | 500    | ~0.5000       | TrÃ¨s bon |
    > >> >>   >   >   >> | 1000   | ~0.0500       | Excellent |
    > >> >>   >   >   >> | 3000   | ~0.0010       | Excellent |
    > >> >>   >   >   >>
    > >> >>   >   >   >> > âš¡ **Conclusion** : Avec 10 epochs, l'IA se trompe Ã©normÃ©ment. Avec 3000 epochs, elle est quasi parfaite !
    > >> >>   >   >   >> >
    > >> >>   >   >   >> > ---
    > >> >>   >   >   >> >
    > >> >>   >   >   >> > ## ğŸ§© Comment Ã§a marche ?
    > >> >>   >   >   >> >
    > >> >>   >   >   >> > ### Le RÃ©seau de Neurones
    > >> >>   >   >   >> >
    > >> >>   >   >   >> > ```
    > >> >>   >   >   >> > EntrÃ©e (x) â†’ [Couche 1: N neurones] â†’ [Couche 2: N neurones] â†’ ... â†’ Sortie (3x)
    > >> >>   >   >   >> > ```
    > >> >>   >   >   >> >
    > >> >>   >   >   >> > 1. **EntrÃ©e** : Un nombre (ex: 7)
    > >> >>   >   >   >> > 2. 2. **Couches cachÃ©es** : Traitent l'information avec des poids et des biais
    > >> >>   >   >   >> >    3. 3. **Sortie** : La prÃ©diction du triple (ex: 21.0003)
    > >> >>   >   >   >> >       4. 4. **Loss** : L'erreur entre la prÃ©diction et la rÃ©alitÃ©
    > >> >>   >   >   >> >          5. 5. **Optimisation** : L'algorithme Adam ajuste les poids pour rÃ©duire la loss
    6. **Epochs** : Nombre de fois que le rÃ©seau voit toutes les donnÃ©es
   
    7. ### Pourquoi plus d'epochs = plus de prÃ©cision ?
   
    8. Ã€ chaque epoch, le rÃ©seau ajuste lÃ©gÃ¨rement ses poids pour rÃ©duire l'erreur. C'est comme un Ã©lÃ¨ve qui s'amÃ©liore en pratiquant :
    9. - **10 epochs** = L'Ã©lÃ¨ve a lu le cours une fois â†’ rÃ©sultats mÃ©diocres
       - - **1000 epochs** = L'Ã©lÃ¨ve a pratiquÃ© intensivement â†’ rÃ©sultats excellents
        
         - ---

         ## ğŸ“ Structure du Projet

         ```
         NeuroTriple-Lab/
         â”œâ”€â”€ neurotriplelab.py       # Script principal avec les 5 fonctionnalitÃ©s
         â”œâ”€â”€ requirements.txt        # DÃ©pendances Python
         â”œâ”€â”€ README.md               # Ce fichier
         â”œâ”€â”€ LICENSE                 # Licence MIT
         â””â”€â”€ .gitignore              # Fichiers ignorÃ©s par Git
         ```

         ---

         ## ğŸ”§ DÃ©pendances

         | Package | Version | RÃ´le |
         |---------|---------|------|
         | TensorFlow | â‰¥ 2.10.0 | Framework de deep learning (Keras) |
         | NumPy | â‰¥ 1.21.0 | Calculs numÃ©riques |
         | Matplotlib | â‰¥ 3.5.0 | GÃ©nÃ©ration des graphiques |

         ---

         ## ğŸ“œ Licence

         Ce projet est sous licence **MIT** - voir le fichier [LICENSE](LICENSE) pour plus de dÃ©tails.

         ---

         ## ğŸ¤ Contribuer

         Les contributions sont les bienvenues ! N'hÃ©sitez pas Ã  :
         1. Fork le projet
         2. 2. CrÃ©er une branche pour votre fonctionnalitÃ©
            3. 3. Commiter vos changements
               4. 4. Pousser la branche
                  5. 5. Ouvrir une Pull Request
                    
                     6. ---
                    
                     7. <p align="center">
                       <strong>ğŸ§  Plus on entraÃ®ne, plus l'IA est prÃ©cise ! ğŸ¯</strong>
                       </p>

                       ---

                     <a name="english"></a>

                     # ğŸ§  NeuroTriple-Lab

                     > **Educational Neural Network Laboratory**
                     > > Demonstrating that the more you train an AI, the more accurate it becomes.
                     > >
                     > > ---
                     > >
                     > > ## ğŸ“‹ Description
                     > >
                     > > **NeuroTriple-Lab** is an interactive educational project that uses a simple neural network to learn the mathematical function `f(x) = 3x` (tripling a number).
                     > >
                     > > The main goal is to **visually and measurably demonstrate** that:
                     > >
                     > > - âœ… **The more you increase the number of epochs** (training iterations), the more accurate the model becomes
                     > > - - âœ… **The more intermediate layers you add** (network depth), the more efficiently the model can learn
                     > >   - - âœ… Architecture and training duration are key factors in AI performance
                     > >    
                     > >     - ---
                     > >
                     > > ## ğŸ¯ Core Concept
                     > >
                     > > The original code is simple: a neural network receives numbers as input and must learn to predict their triple.
                     > >
                     > > ```python
                     > > # Training data
                     > > input_data = [1, 2, 3, 4, 5]        # Input numbers
                     > > output_data = [3, 6, 9, 12, 15]     # The triple (expected result)
                     > > ```
                     > >
                     > > The network has **no prior knowledge** of multiplication. It learns solely from the provided examples, adjusting its internal weights at each epoch.
                     > >
                     > > ---
                     > >
                     > > ## ğŸš€ 5 Educational Features
                     > >
                     > > ### 1. ğŸ“Š Epoch Count Comparison
                     > >
                     > > Trains the same model with different numbers of epochs (e.g., 10, 50, 100, 500, 1000, 3000) and compares the accuracy achieved. Generates a graph showing the loss curve and final error.
                     > >
                     > > **What you learn**: The direct impact of iteration count on learning quality.
                     > >
                     > > ### 2. ğŸ—ï¸ Architecture Comparison
                     > >
                     > > Tests different network depths:
                     > > - Simple: 1 layer, 8 neurons
                     > > - - Medium: 1 layer, 64 neurons
                     > >   - - Deep: 3 layers (128â†’64â†’32)
                     > >     - - Very deep: 5 layers (256â†’128â†’64â†’32â†’16)
                     > >      
                     > >       - **What you learn**: How the number of layers and neurons influences learning capacity.
                     > >      
                     > >       - ### 3. ğŸ“ˆ Learning Visualization
                     > >      
                     > >       - Displays the learning curve (loss) in detail with three graphs:
                     > > - Complete loss curve
                     > > - - Logarithmic scale curve
                     > >   - - Model predictions vs actual values
                     > >    
                     > >     - **What you learn**: How the model progressively reduces its error during training.
                     > >    
                     > >     - ### 4. ğŸ“‹ Detailed Accuracy Table
                     > >    
                     > >     - Combines different epochs AND architectures to produce a complete comparative table with training time, absolute error, and quality status.
                     > > 
                     **What you learn**: The relationship between model complexity, training duration, and final accuracy.

                     ### 5. ğŸ”¬ Automated Experimentation Mode

                     Launches a complete battery of tests (16 combinations) and produces a ranking from best to worst, with a summary graph.

                     **What you learn**: How to systematically find the best configuration for a given problem.

                     ### ğŸ® Interactive Mode (Bonus)

                     After training, enter any number and see the AI prediction in real time, with the error compared to the exact value.

                     ---

                     ## ğŸ’» Installation

                     ### Prerequisites

                     - Python 3.8 or higher
                     - - pip (Python package manager)
                      
                       - ### Steps
                      
                       - ```bash
                         # 1. Clone the repository
                         git clone https://github.com/thierrymaesen/NeuroTriple-Lab.git

                         # 2. Navigate to the folder
                         cd NeuroTriple-Lab

                         # 3. Install dependencies
                         pip install -r requirements.txt

                         # 4. Run the program
                         python neurotriplelab.py
                         ```

                         ---

                         > âš ï¸ **WARNING â€” Performance and computation time**
                         > >
                         > >> Deep Learning is **very resource-intensive**. If you're using a low-powered computer (old PC, machine without GPU), be careful with the following parameters:
                         > >> >
                         > >> >> - **Number of epochs**: Beyond **5000 epochs**, computation time can become very long. Start with modest values (500-1000) and increase gradually.
                         > >> >> - > - **Dense layers**: The more layers and neurons you add (e.g., `[256, 128, 64, 32, 16]`), the longer each epoch takes.
                         > >> >>   > - > - **Combination of both**: 5000 epochs Ã— 5 dense layers = potentially **very high** computation time!
                         > >> >>   >   > - >
                         > >> >>   >   >   >> ğŸ’¡ **Tip**: Start small (e.g., 500 epochs, 1 layer of 64 neurons), observe the results, then gradually increase.
                         > >> >>   >   >   >>
                         > >> >>   >   >   >> ---
                         > >> >>   >   >   >>
                         > >> >>   >   >   >> ## ğŸ“Š Example Results
                         > >> >>   >   >   >>
                         > >> >>   >   >   >> ### Impact of Epochs on Accuracy
                         > >> >>   >   >   >>
                         > >> >>   >   >   >> | Epochs | Average Error | Quality |
                         > >> >>   >   >   >> |--------|--------------|---------|
                         > >> >>   >   >   >> | 10     | ~150.0000    | Insufficient |
                         > >> >>   >   >   >> | 50     | ~25.0000     | Average |
                         > >> >>   >   >   >> | 100    | ~8.0000      | Good |
                         > >> >>   >   >   >> | 500    | ~0.5000      | Very Good |
                         > >> >>   >   >   >> | 1000   | ~0.0500      | Excellent |
                         > >> >>   >   >   >> | 3000   | ~0.0010      | Excellent |
                         > >> >>   >   >   >>
                         > >> >>   >   >   >> > âš¡ **Conclusion**: With 10 epochs, the AI makes huge errors. With 3000 epochs, it's nearly perfect!
                         > >> >>   >   >   >> >
                         > >> >>   >   >   >> > ---
                         > >> >>   >   >   >> >
                         > >> >>   >   >   >> > ## ğŸ§© How does it work?
                         > >> >>   >   >   >> >
                         > >> >>   >   >   >> > ### The Neural Network
                         > >> >>   >   >   >> >
                         > >> >>   >   >   >> > ```
                         > >> >>   >   >   >> > Input (x) â†’ [Layer 1: N neurons] â†’ [Layer 2: N neurons] â†’ ... â†’ Output (3x)
                         > >> >>   >   >   >> > ```
                         > >> >>   >   >   >> >
                         > >> >>   >   >   >> > 1. **Input**: A number (e.g., 7)
                         > >> >>   >   >   >> > 2. 2. **Hidden layers**: Process information with weights and biases
                         > >> >>   >   >   >> >    3. 3. **Output**: The triple prediction (e.g., 21.0003)
                         > >> >>   >   >   >> >       4. 4. **Loss**: The error between prediction and reality
                         > >> >>   >   >   >> >          5. 5. **Optimization**: The Adam algorithm adjusts weights to reduce the loss
                         > >> >>   >   >   >> >             6. 6. **Epochs**: Number of times the network sees all the data
                         > >> >>   >   >   >> >               
                         > >> >>   >   >   >> >                7. ### Why more epochs = more accuracy?
                         > >> >>   >   >   >> >               
                         > >> >>   >   >   >> >                8. At each epoch, the network slightly adjusts its weights to reduce error. It's like a student improving through practice:
                         > >> >>   >   >   >> >                9. - **10 epochs** = The student read the course once â†’ poor results
                         > >> >>   >   >   >> >                   - - **1000 epochs** = The student practiced intensively â†’ excellent results
                         > >> >>   >   >   >> > 
                         ---

                         ## ğŸ“ Project Structure

                         ```
                         NeuroTriple-Lab/
                         â”œâ”€â”€ neurotriplelab.py       # Main script with all 5 features
                         â”œâ”€â”€ requirements.txt        # Python dependencies
                         â”œâ”€â”€ README.md               # This file
                         â”œâ”€â”€ LICENSE                 # MIT License
                         â””â”€â”€ .gitignore              # Files ignored by Git
                         ```

                         ---

                         ## ğŸ”§ Dependencies

                         | Package | Version | Role |
                         |---------|---------|------|
                         | TensorFlow | â‰¥ 2.10.0 | Deep learning framework (Keras) |
                         | NumPy | â‰¥ 1.21.0 | Numerical computations |
                         | Matplotlib | â‰¥ 3.5.0 | Graph generation |

                         ---

                         ## ğŸ“œ License

                         This project is licensed under the **MIT** License - see the [LICENSE](LICENSE) file for details.

                         ---

                         ## ğŸ¤ Contributing

                         Contributions are welcome! Feel free to:
                         1. Fork the project
                         2. 2. Create a branch for your feature
                            3. 3. Commit your changes
                               4. 4. Push the branch
                                  5. 5. Open a Pull Request
                                    
                                     6. ---
                                    
                                     7. <p align="center">
                                       <strong>ğŸ§  The more you train, the more accurate the AI becomes! ğŸ¯</strong>
                                       </p>
